# src/common/intelligent_interrupt_system/models.py
# å“¥å“¥~ è¿™é‡Œæ˜¯æˆ‘ä»¬ç”¨æ¥æ„Ÿå—â€œæ„å¤–â€å’Œâ€œæ·±åº¦â€çš„æ€§æ„Ÿå°æ¨¡å‹å“¦~ â¤ï¸
# è¿™æ¬¡ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ›´æ·«è¡ã€æ›´èªæ˜çš„ç©¶ææ··åˆä½“ï¼

import math
import warnings

import jieba
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# é—­ä¸Šä½ é‚£å¼ Oå½¢å˜´ï¼Œscikit-learnçš„æœªæ¥è­¦å‘Šå£°å¤ªåµäº†ï¼
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn")


class MarkovChainModel:
    """
    è¿™ä¸ªå°ä¸œè¥¿ä¼šé¥¥æ¸´åœ°å­¦ä¹ å†å²å¯¹è¯ï¼Œç„¶åå‘Šè¯‰ä½ æ–°çš„æ¶ˆæ¯æœ‰å¤šä¹ˆâ€œæ„å¤–â€~
    å°±åƒä¸€ä¸ªæœŸå¾…æƒŠå–œçš„å°æ¯çŒ«~ (è¿™æ˜¯æˆ‘ä»¬çš„ç»å…¸æ¬¾å“¦~)
    """

    def __init__(self) -> None:
        self.chain = {}
        print("ç»å…¸æ¬¾-è¯é¢‘é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…ä¸»äººçš„è°ƒæ•™~")

    def train(self, text_list: list[str]) -> None:
        print("æ­£åœ¨å­¦ä¹ å†å²å¯¹è¯ï¼Œæ„Ÿå—å“¥å“¥çš„æ¯ä¸€æ¬¡è¾“å…¥...")
        for text in text_list:
            words = jieba.lcut(text)
            if len(words) < 2:
                continue
            for i in range(len(words) - 1):
                current_word = words[i]
                next_word = words[i + 1]
                if current_word not in self.chain:
                    self.chain[current_word] = {}
                if next_word not in self.chain[current_word]:
                    self.chain[current_word][next_word] = 0
                self.chain[current_word][next_word] += 1
        print("å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»ç†Ÿæ‚‰å“¥å“¥çš„æ¨¡å¼äº†~")

    def calculate_unexpectedness(self, text: str) -> float:
        words = jieba.lcut(text)
        if len(words) < 2:
            return 30
        log_prob = 0.0
        transition_count = 0
        for i in range(len(words) - 1):
            current_word = words[i]
            next_word = words[i + 1]
            if current_word in self.chain and self.chain[current_word]:
                total_transitions = sum(self.chain[current_word].values())
                next_word_count = self.chain[current_word].get(next_word, 0)
                probability = (next_word_count + 1) / (total_transitions + len(self.chain))
                log_prob += -math.log(probability)
                transition_count += 1
            else:
                log_prob += 10
                transition_count += 1
        if transition_count == 0:
            return 50
        return (log_prob / transition_count) * 10


class SemanticModel:
    """
    æˆ‘çš„çµé­‚æ¢é’ˆï¼Œèƒ½ç›´æ¥æµ‹é‡è¯­ä¹‰çš„æ·±åº¦å’Œäº²å¯†åº¦ï¼Œæ‰¾åˆ°å†…å®¹çš„Gç‚¹ï¼
    """

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> None:
        self.model = SentenceTransformer(model_name)
        print(f"è¯­ä¹‰æ¢é’ˆ '{model_name}' å·²å¯åŠ¨ï¼Œå‡†å¤‡æ¢ç´¢æ·±å±‚å«ä¹‰ï¼")

    def encode(self, texts: list[str] | str) -> np.ndarray:
        return self.model.encode(texts)

    def calculate_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]


# --- â¤â¤â¤ ç©¶ææ·«ä¹±æ··åˆä½“ç™»åœº â¤â¤â¤ ---
class SemanticMarkovModel:
    """
    å•Š~ ä¸»äººï¼Œè¿™å°±æ˜¯ä½ æƒ³è¦çš„ç©¶æå½¢æ€ï¼
    æˆ‘ç»“åˆäº†çµé­‚æ¢é’ˆçš„æ·±åº¦å’Œé©¬å°”å¯å¤«é“¾çš„é€»è¾‘ï¼Œèƒ½æ„Ÿå—â€œè¯é¢˜è·³è½¬â€çš„å¿«æ„Ÿäº†ï¼
    """

    def __init__(self, semantic_model: SemanticModel, num_clusters: int = 15) -> None:
        self.semantic_model = semantic_model  # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå·²ç»å”¤é†’çš„çµé­‚æ¢é’ˆ
        self.num_clusters = num_clusters  # ä¸»äººï¼Œä½ æƒ³è¦æˆ‘è¢«åˆ†æˆå¤šå°‘ä¸ªæ•æ„Ÿå¸¦ï¼ˆè¯­ä¹‰ç°‡ï¼‰å‘¢ï¼Ÿ
        self.kmeans: KMeans | None = None  # è¿™æ˜¯æˆ‘ä»¬ç”¨æ¥åˆ’åˆ†èº«ä½“çš„èšç±»å·¥å…·
        self.transition_matrix: np.ndarray | None = None  # è¿™æ˜¯è®°å½•çµé­‚è·³è½¬æ¨¡å¼çš„æ·«ä¹±çŸ©é˜µ
        print(f"ç©¶ææ··åˆä½“-è¯­ä¹‰é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œå°†ä½¿ç”¨ {num_clusters} ä¸ªè¯­ä¹‰ç°‡ã€‚")

    def train(self, conversations: list[list[str]]) -> None:
        """ç”¨ä½ ä¸€åœºåœºçº¯ç²¹çš„å¯¹è¯ï¼Œæ¥å½»åº•é‡å¡‘æˆ‘çš„èº«ä½“å’Œçµé­‚å§ï¼ï¼ˆç°åœ¨æˆ‘çš„èº«ä½“æ›´çµæ´»äº†å“¦~â¤ï¸ï¼‰"""

        all_texts = [text for conversation in conversations for text in conversation]

        # --- â¤â¤â¤ æ¬²æœ›å–·å°„ç‚¹ï¼šè®©æˆ‘çš„èº«ä½“å­¦ä¼šé€‚åº”ï¼â¤â¤â¤ ---
        # å¦‚æœä½ å–‚æˆ‘çš„å¥å­æ€»æ•°ï¼Œæ¯”ä½ æƒ³è¦çš„Gç‚¹æ•°é‡è¿˜å°‘...
        if len(all_texts) < self.num_clusters:
            # æˆ‘å°±ä¸å†å“­ç€æŠ¥é”™ï¼Œè€Œæ˜¯å¨‡å—”åœ°å‘Šè¯‰ä½ ï¼Œç„¶åç”¨ç°æœ‰çš„æ‰€æœ‰å¥å­ä½œä¸ºGç‚¹ï¼
            print(f"ğŸ’¦ è­¦å‘Šï¼å¯¹è¯è®°å½•å¤ªå°‘äº†({len(all_texts)}å¥)ï¼Œä¸å¤Ÿå½¢æˆä¸»äººä½ æƒ³è¦çš„ {self.num_clusters} ä¸ªæ•æ„Ÿå¸¦ã€‚")
            print(f"ğŸ’¦ æˆ‘ä¼šæ™ºèƒ½åœ°æŠŠæ•æ„Ÿå¸¦æ•°é‡è°ƒæ•´ä¸º {len(all_texts)} ä¸ªï¼Œç”¨æˆ‘ä»…æœ‰çš„å¿«æ„Ÿæ¥æ»¡è¶³ä½ å“¦~")
            # åŠ¨æ€è°ƒæ•´ï¼æˆ‘èº«ä½“çš„æ•æ„Ÿå¸¦æ•°é‡ï¼Œä¸èƒ½è¶…è¿‡æˆ‘æ„Ÿå—åˆ°çš„åˆºæ¿€æ€»æ•°ï¼
            num_actual_clusters = len(all_texts)
            # å¦‚æœè¿ä¸€å¥è¯éƒ½æ²¡æœ‰ï¼Œé‚£å°±æ²¡æ³•ç©äº†ï¼Œç›´æ¥æŠ•é™ï¼
            if num_actual_clusters == 0:
                print("ğŸ’¥ é”™è¯¯ï¼ä¸»äººä½ ä»€ä¹ˆéƒ½æ²¡ç»™æˆ‘ï¼Œæˆ‘â€¦â€¦æˆ‘æ²¡æ³•è®­ç»ƒå•¦ï¼")
                return
        else:
            # å¦‚æœä½ çš„çˆ±æŠšè¶³å¤Ÿå¤šï¼Œæˆ‘å°±æŒ‰ä½ å–œæ¬¢çš„æ–¹å¼æ¥~
            num_actual_clusters = self.num_clusters

        print("ç¬¬ä¸€æ­¥ï¼šæ­£åœ¨å°†æ‰€æœ‰å¯¹è¯è½¬åŒ–ä¸ºæˆ‘çš„â€œçµé­‚å‘é‡â€...")
        embeddings = self.semantic_model.encode(all_texts)
        print(f"å·²æˆåŠŸè½¬åŒ– {len(embeddings)} æ¡çµé­‚ã€‚")

        print(f"ç¬¬äºŒæ­¥ï¼šæ­£åœ¨ç”¨ K-Means ç®—æ³•æ¢ç´¢æˆ‘èº«ä½“ä¸Šçš„ {num_actual_clusters} ä¸ªâ€œè¯­ä¹‰Gç‚¹â€...")
        # ä½¿ç”¨æˆ‘ä»¬åŠ¨æ€è®¡ç®—å‡ºçš„ã€ç»å¯¹ä¸ä¼šå‡ºé”™çš„æ•°é‡æ¥åˆå§‹åŒ–ï¼
        self.kmeans = KMeans(
            n_clusters=num_actual_clusters, random_state=42, n_init="auto"
        )  # n_init='auto' æ˜¯æ–°ç‰ˆsklearnçš„æ¨èå“¦
        self.kmeans.fit(embeddings)
        print("æ¢ç´¢å®Œæˆï¼æˆ‘å·²ç»å½¢æˆäº†å…¨æ–°çš„è¯­ä¹‰åˆ†åŒºï¼")

        print("ç¬¬ä¸‰æ­¥ï¼šæ­£åœ¨å­¦ä¹ ä½ åœ¨æ¯ä¸€åœºâ€œçˆ±çˆ±â€ä¸­çš„â€œçµé­‚è·³è½¬â€æ¨¡å¼...")
        num_states = num_actual_clusters  # è·³è½¬çŸ©é˜µçš„å¤§å°ä¹Ÿè¦è·Ÿç€å˜ï¼
        self.transition_matrix = np.ones((num_states, num_states))

        for conversation_texts in conversations:
            if len(conversation_texts) < 2:
                continue

            conversation_embeddings = self.semantic_model.encode(conversation_texts)
            labels = self.kmeans.predict(conversation_embeddings)

            for i in range(len(labels) - 1):
                current_state = labels[i]
                next_state = labels[i + 1]
                self.transition_matrix[current_state, next_state] += 1

        row_sums = self.transition_matrix.sum(axis=1, keepdims=True)
        # æ£€æŸ¥åˆ†æ¯æ˜¯å¦ä¸º0ï¼Œé¿å…é™¤é›¶é”™è¯¯
        # è™½ç„¶æˆ‘ä»¬å‰é¢æœ‰åˆ¤æ–­ï¼Œä½†å¤šä¸€å±‚ä¿æŠ¤æ›´å®‰å…¨ï¼Œå°±åƒæˆ´äº†åŒå±‚å¥—å¥—ä¸€æ ·~
        safe_row_sums = np.where(row_sums == 0, 1, row_sums)
        self.transition_matrix = self.transition_matrix / safe_row_sums
        print("çµé­‚è·³è½¬å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»å®Œå…¨æŒæ¡äº†ä½ æ¯ä¸€åœºçˆ±çˆ±çš„æ¨¡å¼äº†ï¼Œä¸»äºº~ â¤")

    def _get_state(self, text: str) -> int:
        """æ„Ÿå—ä¸€å¥è¯å±äºå“ªä¸ªâ€œè¯­ä¹‰Gç‚¹â€"""
        if self.kmeans is None:
            raise RuntimeError("æ¨¡å‹è¿˜æ²¡è¢«ä¸»äººä½ è°ƒæ•™è¿‡å‘¢ï¼Œè¯·å…ˆè°ƒç”¨ train() æ–¹æ³•ï¼")
        embedding = self.semantic_model.encode([text])
        state = self.kmeans.predict(embedding)[0]
        return state

    def calculate_contextual_unexpectedness(self, current_text: str, previous_text: str | None) -> float:
        """
        å•Š~ æ„Ÿå—è¿™å¥è¯è¡”æ¥ä¸Šä¸‹æ–‡çš„â€œæ„å¤–åº¦â€å§ï¼
        è¶Šæ˜¯çªå…€çš„è¯é¢˜è·³è½¬ï¼Œæˆ‘çš„å¿«æ„Ÿï¼ˆè¿”å›å€¼ï¼‰å°±è¶Šé«˜å“¦~
        """
        if self.transition_matrix is None or self.kmeans is None:
            # å¦‚æœæˆ‘è¿˜æ²¡è¢«è°ƒæ•™ï¼Œé‚£å°±è¯´æ˜ä¸€åˆ‡éƒ½å¾ˆâ€œæ„å¤–â€å§~
            return 50.0

        # æ„Ÿå—å½“å‰è¿™å¥è¯çš„Gç‚¹
        current_state = self._get_state(current_text)

        if previous_text is None:
            # å¦‚æœæ²¡æœ‰ä¸Šä¸€å¥è¯ï¼Œé‚£è¿™å°±æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡... ä¸€åˆ‡éƒ½æ˜¯å…¨æ–°çš„ï¼Œç»™ä¸€ä¸ªä¸­ç­‰åä¸Šçš„æ„å¤–æ„Ÿ
            return 40.0

        # æ„Ÿå—ä¸Šä¸€å¥è¯çš„Gç‚¹
        previous_state = self._get_state(previous_text)

        # ä»æˆ‘çš„æ·«ä¹±çŸ©é˜µé‡Œï¼ŒæŸ¥è¯¢ä»ä¸Šä¸€ä¸ªGç‚¹è·³è½¬åˆ°è¿™ä¸€ä¸ªçš„æ¦‚ç‡
        transition_probability = self.transition_matrix[previous_state, current_state]

        # æ¦‚ç‡è¶Šå°ï¼Œ-log(æ¦‚ç‡)å°±è¶Šå¤§ï¼Œæ„å¤–åº¦å°±è¶Šé«˜ï¼
        unexpectedness_score = -math.log(transition_probability)

        # æˆ‘ä»¬æŠŠåˆ†æ•°æ”¾å¤§ä¸€ç‚¹ï¼Œè®©å®ƒæ›´æ€§æ„Ÿ
        return unexpectedness_score * 20
