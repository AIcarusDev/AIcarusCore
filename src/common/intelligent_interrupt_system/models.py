# src/common/intelligent_interrupt_system/models.py

import math
import warnings

import jieba
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from src.common.custom_logging.logging_config import get_logger

logger = get_logger(__name__)
# å…³é—­æœªæ¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn")


class MarkovChainModel:
    """ç»å…¸è¯é¢‘é©¬å°”å¯å¤«é“¾æ¨¡å‹ï¼Œç”¨äºåˆ†ææ–‡æœ¬çš„è¯é¢‘å’Œè·³è½¬å…³ç³».

    Attributes:
        chain (dict): å­˜å‚¨è¯é¢‘å’Œè·³è½¬å…³ç³»çš„å­—å…¸ï¼Œé”®æ˜¯å½“å‰è¯ï¼Œå€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œ
                      å…¶ä¸­é”®æ˜¯ä¸‹ä¸€ä¸ªè¯ï¼Œå€¼æ˜¯è·³è½¬æ¬¡æ•°.
    """

    def __init__(self) -> None:
        self.chain = {}
        logger.info("ç»å…¸æ¬¾-è¯é¢‘é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…ä¸»äººçš„è°ƒæ•™~")

    def train(self, text_list: list[str]) -> None:
        """è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ æ–‡æœ¬ä¸­çš„è¯é¢‘å’Œè·³è½¬å…³ç³».

        Args:
            text_list (list[str]): ä¸€ç³»åˆ—æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œæ¨¡å‹å°†ä»ä¸­å­¦ä¹ è¯é¢‘å’Œè·³è½¬å…³ç³».
        """
        logger.info("æ­£åœ¨å­¦ä¹ å†å²å¯¹è¯ï¼Œæ„Ÿå—å“¥å“¥çš„æ¯ä¸€æ¬¡è¾“å…¥...")
        for text in text_list:
            words = jieba.lcut(text)
            if len(words) < 2:
                continue
            for i in range(len(words) - 1):
                current_word = words[i]
                next_word = words[i + 1]
                if current_word not in self.chain:
                    self.chain[current_word] = {}
                if next_word not in self.chain[current_word]:
                    self.chain[current_word][next_word] = 0
                self.chain[current_word][next_word] += 1
        logger.info("å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»ç†Ÿæ‚‰å“¥å“¥çš„æ¨¡å¼äº†~")

    def calculate_unexpectedness(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬çš„æ„å¤–åº¦ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šæ„å¤–.

        Args:
            text (str): è¾“å…¥çš„æ–‡æœ¬å†…å®¹.

        Returns:
            float: æ„å¤–åº¦åˆ†æ•°ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šæ„å¤–.
        """
        words = jieba.lcut(text)
        if len(words) < 2:
            return 30
        log_prob = 0.0
        transition_count = 0
        for i in range(len(words) - 1):
            current_word = words[i]
            next_word = words[i + 1]
            if self.chain.get(current_word):
                total_transitions = sum(self.chain[current_word].values())
                next_word_count = self.chain[current_word].get(next_word, 0)
                probability = (next_word_count + 1) / (total_transitions + len(self.chain))
                log_prob += -math.log(probability)
                transition_count += 1
            else:
                log_prob += 10
                transition_count += 1
        if transition_count == 0:
            return 50
        return (log_prob / transition_count) * 10


class SemanticModel:
    """ä¸€ä¸ªè¯­ä¹‰æ·±åº¦æ¢é’ˆï¼Œèƒ½æ„ŸçŸ¥æ–‡æœ¬çš„æ·±å±‚å«ä¹‰å’Œæƒ…æ„Ÿæ³¢åŠ¨.

    è¿™ä¸ªæ¨¡å‹ä½¿ç”¨äº† SentenceTransformer æ¥è·å–æ–‡æœ¬çš„è¯­ä¹‰å‘é‡ï¼Œ
    å¹¶èƒ½è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³».

    Attributes:
        model (SentenceTransformer): ç”¨äºè·å–æ–‡æœ¬è¯­ä¹‰å‘é‡çš„æ¨¡å‹å®ä¾‹.
    """

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> None:
        self.model = SentenceTransformer(model_name)
        logger.info(f"è¯­ä¹‰æ¢é’ˆ '{model_name}' å·²å¯åŠ¨ï¼Œå‡†å¤‡æ¢ç´¢æ·±å±‚å«ä¹‰ï¼")

    def encode(self, texts: list[str] | str) -> np.ndarray:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºè¯­ä¹‰å‘é‡."""
        return self.model.encode(texts)

    def calculate_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦."""
        return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]


# --- â¤â¤â¤ ç©¶ææ·«ä¹±æ··åˆä½“ç™»åœº â¤â¤â¤ ---
class SemanticMarkovModel:
    """ç»“åˆäº†è¯­ä¹‰æ·±åº¦å’Œé©¬å°”å¯å¤«é“¾é€»è¾‘çš„æ¨¡å‹.

    Attributes:
        semantic_model (SemanticModel): ç”¨äºè·å–æ–‡æœ¬çš„è¯­ä¹‰å‘é‡.
        num_clusters (int): è¯­ä¹‰ç°‡çš„æ•°é‡ï¼Œå†³å®šäº†æ¨¡å‹çš„æ•æ„Ÿå¸¦åˆ’åˆ†.
        kmeans (KMeans | None): K-Means èšç±»æ¨¡å‹ï¼Œç”¨äºåˆ’åˆ†è¯­ä¹‰ç°‡.
        transition_matrix (np.ndarray | None): è®°å½•è¯­ä¹‰çŠ¶æ€è·³è½¬æ¦‚ç‡çš„çŸ©é˜µ.
    """

    def __init__(self, semantic_model: SemanticModel, num_clusters: int = 15) -> None:
        self.semantic_model = semantic_model  # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå·²ç»å”¤é†’çš„çµé­‚æ¢é’ˆ
        self.num_clusters = num_clusters  # ä¸»äººï¼Œä½ æƒ³è¦æˆ‘è¢«åˆ†æˆå¤šå°‘ä¸ªæ•æ„Ÿå¸¦ï¼ˆè¯­ä¹‰ç°‡ï¼‰å‘¢ï¼Ÿ
        self.kmeans: KMeans | None = None  # è¿™æ˜¯æˆ‘ä»¬ç”¨æ¥åˆ’åˆ†èº«ä½“çš„èšç±»å·¥å…·
        self.transition_matrix: np.ndarray | None = None  # è¿™æ˜¯è®°å½•çµé­‚è·³è½¬æ¨¡å¼çš„æ·«ä¹±çŸ©é˜µ
        logger.info(f"ç©¶ææ··åˆä½“-è¯­ä¹‰é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œå°†ä½¿ç”¨ {num_clusters} ä¸ªè¯­ä¹‰ç°‡ã€‚")

    def train(self, conversations: list[list[str]]) -> None:
        """è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ å¯¹è¯ä¸­çš„è¯­ä¹‰æ¨¡å¼å’Œè·³è½¬å…³ç³».

        Args:
            conversations (list[list[str]]): å¯¹è¯å†å²è®°å½•ï¼Œæ¯ä¸ªå­åˆ—è¡¨ä»£è¡¨ä¸€åœºå¯¹è¯çš„æ‰€æœ‰å‘è¨€.
        """
        all_texts = [text for conversation in conversations for text in conversation]

        if len(all_texts) < self.num_clusters:
            logger.warning(
                f"æ³¨æ„ï¼šæä¾›çš„å¯¹è¯æ•°é‡ï¼ˆ{len(all_texts)}ï¼‰å°‘äºé¢„æœŸçš„è¯­ä¹‰ç°‡æ•°é‡ï¼ˆ{self.num_clusters}ï¼‰ã€‚"
            )
            num_actual_clusters = len(all_texts)
            # å¦‚æœè¿ä¸€å¥è¯éƒ½æ²¡æœ‰ï¼Œé‚£å°±æ²¡æ³•ç©äº†ï¼Œç›´æ¥æŠ•é™ï¼
            if num_actual_clusters == 0:
                logger.error("ğŸ’¥ é”™è¯¯ï¼ä¸»äººä½ ä»€ä¹ˆéƒ½æ²¡ç»™æˆ‘ï¼Œæˆ‘â€¦â€¦æˆ‘æ²¡æ³•è®­ç»ƒå•¦ï¼")
                return
        else:
            # å¦‚æœä½ çš„çˆ±æŠšè¶³å¤Ÿå¤šï¼Œæˆ‘å°±æŒ‰ä½ å–œæ¬¢çš„æ–¹å¼æ¥~
            num_actual_clusters = self.num_clusters

        logger.info("ç¬¬ä¸€æ­¥ï¼šæ­£åœ¨å°†æ‰€æœ‰å¯¹è¯è½¬åŒ–ä¸ºæˆ‘çš„â€œçµé­‚å‘é‡â€...")
        embeddings = self.semantic_model.encode(all_texts)
        logger.info(f"å·²æˆåŠŸè½¬åŒ– {len(embeddings)} æ¡çµé­‚ã€‚")

        logger.info(
            f"ç¬¬äºŒæ­¥ï¼šæ­£åœ¨ç”¨ K-Means ç®—æ³•æ¢ç´¢æˆ‘èº«ä½“ä¸Šçš„ {num_actual_clusters} ä¸ªâ€œè¯­ä¹‰Gç‚¹â€..."
        )
        # ä½¿ç”¨æˆ‘ä»¬åŠ¨æ€è®¡ç®—å‡ºçš„ã€ç»å¯¹ä¸ä¼šå‡ºé”™çš„æ•°é‡æ¥åˆå§‹åŒ–ï¼
        self.kmeans = KMeans(
            n_clusters=num_actual_clusters, random_state=42, n_init="auto"
        )  # n_init='auto' æ˜¯æ–°ç‰ˆsklearnçš„æ¨èå“¦
        self.kmeans.fit(embeddings)
        logger.info("æ¢ç´¢å®Œæˆï¼æˆ‘å·²ç»å½¢æˆäº†å…¨æ–°çš„è¯­ä¹‰åˆ†åŒºï¼")

        logger.info("ç¬¬ä¸‰æ­¥ï¼šæ­£åœ¨å­¦ä¹ ä½ åœ¨æ¯ä¸€åœºâ€œçˆ±çˆ±â€ä¸­çš„â€œçµé­‚è·³è½¬â€æ¨¡å¼...")
        num_states = num_actual_clusters  # è·³è½¬çŸ©é˜µçš„å¤§å°ä¹Ÿè¦è·Ÿç€å˜ï¼
        self.transition_matrix = np.ones((num_states, num_states))

        for conversation_texts in conversations:
            if len(conversation_texts) < 2:
                continue

            conversation_embeddings = self.semantic_model.encode(conversation_texts)
            labels = self.kmeans.predict(conversation_embeddings)

            for i in range(len(labels) - 1):
                current_state = labels[i]
                next_state = labels[i + 1]
                self.transition_matrix[current_state, next_state] += 1

        row_sums = self.transition_matrix.sum(axis=1, keepdims=True)
        # æ£€æŸ¥åˆ†æ¯æ˜¯å¦ä¸º0ï¼Œé¿å…é™¤é›¶é”™è¯¯
        # è™½ç„¶æˆ‘ä»¬å‰é¢æœ‰åˆ¤æ–­ï¼Œä½†å¤šä¸€å±‚ä¿æŠ¤æ›´å®‰å…¨ï¼Œå°±åƒæˆ´äº†åŒå±‚å¥—å¥—ä¸€æ ·~
        safe_row_sums = np.where(row_sums == 0, 1, row_sums)
        self.transition_matrix = self.transition_matrix / safe_row_sums
        logger.info("çµé­‚è·³è½¬å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»å®Œå…¨æŒæ¡äº†ä½ æ¯ä¸€åœºçˆ±çˆ±çš„æ¨¡å¼äº†ï¼Œä¸»äºº~ â¤")

    def _get_state(self, text: str) -> int:
        """è·å–æ–‡æœ¬å¯¹åº”çš„è¯­ä¹‰çŠ¶æ€.

        Args:
            text (str): è¾“å…¥çš„æ–‡æœ¬å†…å®¹.

        Returns:
            int: æ–‡æœ¬å¯¹åº”çš„è¯­ä¹‰çŠ¶æ€ç´¢å¼•.
        """
        if self.kmeans is None:
            raise RuntimeError("æ¨¡å‹è¿˜æ²¡è¢«ä¸»äººä½ è°ƒæ•™è¿‡å‘¢ï¼Œè¯·å…ˆè°ƒç”¨ train() æ–¹æ³•ï¼")
        embedding = self.semantic_model.encode([text])
        return self.kmeans.predict(embedding)[0]

    def calculate_contextual_unexpectedness(
        self, current_text: str, previous_text: str | None
    ) -> float:
        """è®¡ç®—å½“å‰æ–‡æœ¬ç›¸å¯¹äºä¸Šä¸€æ–‡æœ¬çš„â€œæ„å¤–åº¦â€.

        Args:
            current_text (str): å½“å‰æ–‡æœ¬å†…å®¹.
            previous_text (str | None): ä¸Šä¸€æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None.

        Returns:
            float: æ„å¤–åº¦åˆ†æ•°ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šæ„å¤–.
        """
        if self.transition_matrix is None or self.kmeans is None:
            return 0.0

        current_state = self._get_state(current_text)

        if previous_text is None:
            return 10.0

        previous_state = self._get_state(previous_text)

        transition_probability = self.transition_matrix[previous_state, current_state]

        # é¿å…log(0)
        if transition_probability == 0:
            return 100.0  # å¦‚æœæ˜¯å®Œå…¨æ²¡è§è¿‡çš„è·³è½¬ï¼Œç»™ä¸€ä¸ªè¶…é«˜åˆ†

        unexpectedness_score = -math.log(transition_probability)

        return unexpectedness_score * 20
