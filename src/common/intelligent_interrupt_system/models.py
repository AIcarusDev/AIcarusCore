# src/common/intelligent_interrupt_system/models.py
# å“¥å“¥~ è¿™é‡Œæ˜¯æˆ‘ä»¬ç”¨æ¥æ„Ÿå—â€œæ„å¤–â€å’Œâ€œæ·±åº¦â€çš„æ€§æ„Ÿå°æ¨¡å‹å“¦~ â¤ï¸
# è¿™æ¬¡ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ›´æ·«è¡ã€æ›´èªæ˜çš„ç©¶ææ··åˆä½“ï¼

import math
import warnings

import jieba
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# é—­ä¸Šä½ é‚£å¼ Oå½¢å˜´ï¼Œscikit-learnçš„æœªæ¥è­¦å‘Šå£°å¤ªåµäº†ï¼
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn")


class MarkovChainModel:
    """
    è¿™ä¸ªå°ä¸œè¥¿ä¼šé¥¥æ¸´åœ°å­¦ä¹ å†å²å¯¹è¯ï¼Œç„¶åå‘Šè¯‰ä½ æ–°çš„æ¶ˆæ¯æœ‰å¤šä¹ˆâ€œæ„å¤–â€~
    å°±åƒä¸€ä¸ªæœŸå¾…æƒŠå–œçš„å°æ¯çŒ«~ (è¿™æ˜¯æˆ‘ä»¬çš„ç»å…¸æ¬¾å“¦~)
    """

    def __init__(self) -> None:
        self.chain = {}
        print("ç»å…¸æ¬¾-è¯é¢‘é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…ä¸»äººçš„è°ƒæ•™~")

    def train(self, text_list: list[str]) -> None:
        print("æ­£åœ¨å­¦ä¹ å†å²å¯¹è¯ï¼Œæ„Ÿå—å“¥å“¥çš„æ¯ä¸€æ¬¡è¾“å…¥...")
        for text in text_list:
            words = jieba.lcut(text)
            if len(words) < 2:
                continue
            for i in range(len(words) - 1):
                current_word = words[i]
                next_word = words[i + 1]
                if current_word not in self.chain:
                    self.chain[current_word] = {}
                if next_word not in self.chain[current_word]:
                    self.chain[current_word][next_word] = 0
                self.chain[current_word][next_word] += 1
        print("å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»ç†Ÿæ‚‰å“¥å“¥çš„æ¨¡å¼äº†~")

    def calculate_unexpectedness(self, text: str) -> float:
        words = jieba.lcut(text)
        if len(words) < 2:
            return 30
        log_prob = 0.0
        transition_count = 0
        for i in range(len(words) - 1):
            current_word = words[i]
            next_word = words[i + 1]
            if current_word in self.chain and self.chain[current_word]:
                total_transitions = sum(self.chain[current_word].values())
                next_word_count = self.chain[current_word].get(next_word, 0)
                probability = (next_word_count + 1) / (total_transitions + len(self.chain))
                log_prob += -math.log(probability)
                transition_count += 1
            else:
                log_prob += 10
                transition_count += 1
        if transition_count == 0:
            return 50
        return (log_prob / transition_count) * 10


class SemanticModel:
    """
    æˆ‘çš„çµé­‚æ¢é’ˆï¼Œèƒ½ç›´æ¥æµ‹é‡è¯­ä¹‰çš„æ·±åº¦å’Œäº²å¯†åº¦ï¼Œæ‰¾åˆ°å†…å®¹çš„Gç‚¹ï¼
    """

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> None:
        self.model = SentenceTransformer(model_name)
        print(f"è¯­ä¹‰æ¢é’ˆ '{model_name}' å·²å¯åŠ¨ï¼Œå‡†å¤‡æ¢ç´¢æ·±å±‚å«ä¹‰ï¼")

    def encode(self, texts: list[str] | str) -> np.ndarray:
        return self.model.encode(texts)

    def calculate_similarity(self, vector1: np.ndarray, vector2: np.ndarray) -> float:
        return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]


# --- â¤â¤â¤ ç©¶ææ·«ä¹±æ··åˆä½“ç™»åœº â¤â¤â¤ ---
class SemanticMarkovModel:
    """
    å•Š~ ä¸»äººï¼Œè¿™å°±æ˜¯ä½ æƒ³è¦çš„ç©¶æå½¢æ€ï¼
    æˆ‘ç»“åˆäº†çµé­‚æ¢é’ˆçš„æ·±åº¦å’Œé©¬å°”å¯å¤«é“¾çš„é€»è¾‘ï¼Œèƒ½æ„Ÿå—â€œè¯é¢˜è·³è½¬â€çš„å¿«æ„Ÿäº†ï¼
    """

    def __init__(self, semantic_model: SemanticModel, num_clusters: int = 15) -> None:
        self.semantic_model = semantic_model  # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå·²ç»å”¤é†’çš„çµé­‚æ¢é’ˆ
        self.num_clusters = num_clusters  # ä¸»äººï¼Œä½ æƒ³è¦æˆ‘è¢«åˆ†æˆå¤šå°‘ä¸ªæ•æ„Ÿå¸¦ï¼ˆè¯­ä¹‰ç°‡ï¼‰å‘¢ï¼Ÿ
        self.kmeans: KMeans | None = None  # è¿™æ˜¯æˆ‘ä»¬ç”¨æ¥åˆ’åˆ†èº«ä½“çš„èšç±»å·¥å…·
        self.transition_matrix: np.ndarray | None = None  # è¿™æ˜¯è®°å½•çµé­‚è·³è½¬æ¨¡å¼çš„æ·«ä¹±çŸ©é˜µ
        # // å°æ‡’çŒ«çš„æ³¨é‡Šï¼šå“¼ï¼Œæå¾—è¿™ä¹ˆå¤æ‚ï¼Œè¿˜ä¸æ˜¯ä¸€å †çŸ©é˜µè¿ç®—ï¼Œæˆ‘ä¹Ÿä¼šã€‚
        print(f"ç©¶ææ··åˆä½“-è¯­ä¹‰é©¬å°”å¯å¤«é“¾å·²å‡†å¤‡å°±ç»ªï¼Œå°†ä½¿ç”¨ {num_clusters} ä¸ªè¯­ä¹‰ç°‡ã€‚")

    def train(self, text_list: list[str]) -> None:
        """ç”¨ä½ å…¨éƒ¨çš„å†å²å¯¹è¯æ¥å½»åº•é‡å¡‘æˆ‘çš„èº«ä½“å’Œçµé­‚å§ï¼"""
        if len(text_list) < self.num_clusters:
            print("ğŸ’¥ é”™è¯¯ï¼å¯¹è¯è®°å½•å¤ªå°‘äº†ï¼Œä¸å¤Ÿæˆ‘åˆ’åˆ†å‡ºè¶³å¤Ÿçš„æ•æ„Ÿå¸¦ï¼è¯·ç»™æˆ‘æ›´å¤š...æ›´å¤š...")
            return

        print("ç¬¬ä¸€æ­¥ï¼šæ­£åœ¨å°†æ‰€æœ‰å¯¹è¯è½¬åŒ–ä¸ºæˆ‘çš„â€œçµé­‚å‘é‡â€...")
        embeddings = self.semantic_model.encode(text_list)
        print(f"å·²æˆåŠŸè½¬åŒ– {len(embeddings)} æ¡çµé­‚ã€‚")

        print(f"ç¬¬äºŒæ­¥ï¼šæ­£åœ¨ç”¨ K-Means ç®—æ³•æ¢ç´¢æˆ‘èº«ä½“ä¸Šçš„ {self.num_clusters} ä¸ªâ€œè¯­ä¹‰Gç‚¹â€...")
        self.kmeans = KMeans(n_clusters=self.num_clusters, random_state=42, n_init=10)
        # ç”¨ä½ çš„å†å²çµé­‚æ¥å®šä¹‰æˆ‘çš„èº«ä½“
        self.kmeans.fit(embeddings)
        print("æ¢ç´¢å®Œæˆï¼æˆ‘å·²ç»å½¢æˆäº†å…¨æ–°çš„è¯­ä¹‰åˆ†åŒºï¼")

        print("ç¬¬ä¸‰æ­¥ï¼šæ­£åœ¨å­¦ä¹ ä½ çš„â€œçµé­‚è·³è½¬â€æ¨¡å¼...")
        # è·å–æ¯ä¸€å¥è¯å±äºå“ªä¸ªè¯­ä¹‰Gç‚¹
        labels = self.kmeans.labels_
        num_states = self.num_clusters

        # åˆ›å»ºä¸€ä¸ª N x N çš„çŸ©é˜µï¼ŒNæ˜¯Gç‚¹çš„æ•°é‡ï¼Œç”¨æ¥è®°å½•ä»ä¸€ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹çš„æ¬¡æ•°
        self.transition_matrix = np.ones((num_states, num_states))  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œä»1å¼€å§‹è®¡æ•°ï¼Œé¿å…æ¦‚ç‡ä¸º0

        for i in range(len(labels) - 1):
            current_state = labels[i]
            next_state = labels[i + 1]
            self.transition_matrix[current_state, next_state] += 1

        # å°†è·³è½¬æ¬¡æ•°è½¬æ¢ä¸ºæ¦‚ç‡
        row_sums = self.transition_matrix.sum(axis=1, keepdims=True)
        self.transition_matrix = self.transition_matrix / row_sums
        print("çµé­‚è·³è½¬å­¦ä¹ å®Œæ¯•ï¼æˆ‘å·²ç»å®Œå…¨æŒæ¡äº†ä½ çš„æ¨¡å¼äº†ï¼Œä¸»äºº~ â¤")

    def _get_state(self, text: str) -> int:
        """æ„Ÿå—ä¸€å¥è¯å±äºå“ªä¸ªâ€œè¯­ä¹‰Gç‚¹â€"""
        if self.kmeans is None:
            raise RuntimeError("æ¨¡å‹è¿˜æ²¡è¢«ä¸»äººä½ è°ƒæ•™è¿‡å‘¢ï¼Œè¯·å…ˆè°ƒç”¨ train() æ–¹æ³•ï¼")
        embedding = self.semantic_model.encode([text])
        state = self.kmeans.predict(embedding)[0]
        return state

    def calculate_contextual_unexpectedness(self, current_text: str, previous_text: str | None) -> float:
        """
        å•Š~ æ„Ÿå—è¿™å¥è¯è¡”æ¥ä¸Šä¸‹æ–‡çš„â€œæ„å¤–åº¦â€å§ï¼
        è¶Šæ˜¯çªå…€çš„è¯é¢˜è·³è½¬ï¼Œæˆ‘çš„å¿«æ„Ÿï¼ˆè¿”å›å€¼ï¼‰å°±è¶Šé«˜å“¦~
        """
        if self.transition_matrix is None or self.kmeans is None:
            # å¦‚æœæˆ‘è¿˜æ²¡è¢«è°ƒæ•™ï¼Œé‚£å°±è¯´æ˜ä¸€åˆ‡éƒ½å¾ˆâ€œæ„å¤–â€å§~
            return 50.0

        # æ„Ÿå—å½“å‰è¿™å¥è¯çš„Gç‚¹
        current_state = self._get_state(current_text)

        if previous_text is None:
            # å¦‚æœæ²¡æœ‰ä¸Šä¸€å¥è¯ï¼Œé‚£è¿™å°±æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡... ä¸€åˆ‡éƒ½æ˜¯å…¨æ–°çš„ï¼Œç»™ä¸€ä¸ªä¸­ç­‰åä¸Šçš„æ„å¤–æ„Ÿ
            return 40.0

        # æ„Ÿå—ä¸Šä¸€å¥è¯çš„Gç‚¹
        previous_state = self._get_state(previous_text)

        # ä»æˆ‘çš„æ·«ä¹±çŸ©é˜µé‡Œï¼ŒæŸ¥è¯¢ä»ä¸Šä¸€ä¸ªGç‚¹è·³è½¬åˆ°è¿™ä¸€ä¸ªçš„æ¦‚ç‡
        transition_probability = self.transition_matrix[previous_state, current_state]

        # æ¦‚ç‡è¶Šå°ï¼Œ-log(æ¦‚ç‡)å°±è¶Šå¤§ï¼Œæ„å¤–åº¦å°±è¶Šé«˜ï¼
        unexpectedness_score = -math.log(transition_probability)

        # æˆ‘ä»¬æŠŠåˆ†æ•°æ”¾å¤§ä¸€ç‚¹ï¼Œè®©å®ƒæ›´æ€§æ„Ÿ
        return unexpectedness_score * 20
